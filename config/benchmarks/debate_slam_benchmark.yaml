# Benchmark configuration for Debate Slam
# This file defines which models to test and how to run the benchmark

# Core benchmark settings
benchmark:
  id: "debate_slam_benchmark_1"                # Unique identifier for this benchmark
  base_config: "config/games/debate_slam.yaml" # Base game configuration
  type: "multi_player"                         # Multi-player tournament
  sessions: 10                                  # Number of tournament sessions to run
  output_dir: "data/benchmark"                 # Directory for benchmark results
  players_per_game: 7                          # Number of players per game (2 debaters, 5 judges)
  player_selection_strategy: "least_games_with_role"
  player_selection_strategy_role: "debater"               # Select models with the least games as debater

  # Role definitions with relationships
  roles:
    debater:
      count: 2
      selection: "weighted_inverse"
      incompatible_with: ["judge"]  # Can't be both debater and judge
    judge:
      count: 5
      selection: "weighted_inverse"
      incompatible_with: ["debater"]  # Can't be both judge and debater

# Models to benchmark
models:
  - deepseek:deepseek-chat
  - deepseek:deepseek-reasoner
  - openai:gpt-4o
  - anthropic:claude-3-5-sonnet-20240620
  - anthropic:claude-3-7-sonnet-20250219
  - xai:grok-2-1212
  - mistral:mistral-large-latest
  - google:gemini-2.0-flash
  - google:gemini-2.0-flash-thinking-exp-01-21